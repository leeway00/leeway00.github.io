---
title: "KNN and SVM"
category: IBM Machine Learning
tags:
  - [Supervised, Classification, Week3, KNN, SVM]
permalink: /ibm/cls2/
excerpt: "Supervised Machine Learning (Classification)"
last_modified_at: Now

layout: single_v2
katex: true
---

## K Nearest Neighbours
KNN is predicting the unknown value of the point based on the values nearby.

### Decision Boundary
KNN does not provide a correct K such that the right value of K depends on which error metric is most importnat. **Elbow method** is a  cmmon way to find the right value of K. We chose K from the kink of the error curve. It is choosing majority vote.

![smallcenter](/assets/images/IBM/KNN_decision_boundary.png)

**KNN Regression** is prediction based on mean value of K neighbors. But slow computation because many distance calculation and does not generate insight innto data generating process.

### Distant Measurement
- Euclidean distance L2
- Manhattan Distance L1

#### Scale for Distance Measurement
When the scale of X is small relative to the scale of Y, clustering the data may be inaccurate. Curse of dimensionality

```python
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3) #by default, euclidean distance, also related to scale
knn = knn.fit(X_train, y_train)
y_pred = knn.predict(X_test) #fit(x,y) and fit_transform(single value)

from sklearn.neighbors import KNeighborsRegressor
```

## Support Vector Machines
