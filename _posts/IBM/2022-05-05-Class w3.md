---
title: "Decision Tree"
category: IBM Machine Learning
tags:
  - [Supervised, Classification, Week3, Decision Tree]
permalink: /ibm/cls3/
excerpt: "Supervised Machine Learning (Classification)"
last_modified_at: Now

layout: single_v2
katex: true
---

## Decision Tree

Building a decision tree 
1. select a feature
2. split the data into two groups.
3. Split until 
   1. the leaf node are pure (only one class remains)
   2. Maximum depth of the tree is reached
   3. A performance metric is achieved

Decision tree uses greedy search to find the best split at each step. The **Best Split** is the split that minimizes the **information**.\

### Spliting Criteria
Classification Error Equation 

$$E(t) = 1-max_i[P(i|t)]$$

But this metric does not change overall Error rate when parent node error is compared with weighted average of child node error.\

Thus, we use Entroy Equation

$$ H(t) = -\sum_i[P(i|t)log_2(P(i|t))]$$

![small](/images/IBM/DT_error.png)

The Gini index is also used

$$ G(T) = 1-\sum_i P(i|t)^2$$

### Characteristics of a Decision Tree
Since DT easily ovefitted, we use pruning. Prune based on classification error threshold. Pros is that it is easy to interpret and implement.

![small](/images/IBM/DT_purity-error.png)

```python
from sklearn.tree import DecisionTreeClassifier
DTC = DecisionTreeClassifier(criterion='Gini', max_Features=10, max_depth=3)
DTC.fit(X_train, y_train)
y_pred = DTC.predict(X_test)
```

## Ensamble
Combining prediction to reduce variance. We can reduce variance by pruning but pruning may lost generlization

### Bagging
Bagging is bootstrap aggregating. h