---
title: "Decision Tree"
category: IBM Machine Learning
tags:
  - [Supervised, Classification, Week3, Decision Tree]
permalink: /ibm/cls3/
excerpt: "Supervised Machine Learning (Classification)"
last_modified_at: Now

layout: single_v2
katex: true
---

## Decision Tree

Building a decision tree 
1. select a feature
2. split the data into two groups.
3. Split until 
   1. the leaf node are pure (only one class remains)
   2. Maximum depth of the tree is reached
   3. A performance metric is achieved

Decision tree uses greedy search to find the best split at each step. The **Best Split** is the split that minimizes the **information**.\

### Spliting Criteria
Classification Error Equation 

$$E(t) = 1-max_i[P(i|t)]$$

But this metric does not change overall Error rate when parent node error is compared with weighted average of child node error.\

Thus, we use Entroy Equation

$$ H(t) = -\sum_i[P(i|t)log_2(P(i|t))]$$

![small](/images/IBM/DT_error.png)

The Gini index is also used

$$ G(T) = 1-\sum_i P(i|t)^2$$

### Characteristics of a Decision Tree
Since DT easily ovefitted, we use pruning. Prune based on classification error threshold. Pros is that it is easy to interpret and implement.

![smallcenter](/images/IBM/DT_purity-error.png)

```python
from sklearn.tree import DecisionTreeClassifier
DTC = DecisionTreeClassifier(criterion='Gini', max_Features=10, max_depth=3)
DTC.fit(X_train, y_train)
y_pred = DTC.predict(X_test)
```

## Ensamble
Combining prediction to reduce variance. We can reduce variance by pruning but pruning may lost generlization

### Bagging
Bagging is bootstrap aggregating.Bagging is a tree ensemble that combines the prediction of several trees that were trained on bootstrap samples of the data. The underlying idea is that a model that averages the predictions of multiple models reduces the variance of a single model and has high chances to generalize well when scoring new data.\
Heterogeneous input data allowed, no preprocessing needed
```python
from sklearn.ensemble import BaggingClassifier
bc = BaggingClassifier(base_estimator=DTC, n_estimators=10, max_samples=0.5, max_features=0.5)
bc.fit(X_train, y_train)
```

#### Random Forest
The bagged variance is $\sigma^2 /n$. However bootstrap samples are correlated $\rho \sigma^2 + \frac{1-\rho}{n} \sigma^2$.\
We introduce **more randomness????** to solve this. Use random subset of features ofr each tree, so that classification $\sqrt{m}$ amd regression $m/3$. 

![smallcenter](/images/IBM/RFM.png)

```python
from sklearn.ensemble import BaggingClassifier

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=10, max_features=0.5, max_depth=3)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)

from sklearn.ensmable import ExtraTreesClassifier
ec = ExtraTreesClassifier(n_estimators=10, max_features=0.5, max_depth=3)
ec.fit(X_train, y_train)
y_pred = ec.predict(X_test)
```
