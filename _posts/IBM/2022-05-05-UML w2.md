---
title: "Clustering Algorithms"

category: IBM Machine Learning
tags:
  - [Unsupervised, Week2]

layout: single_v2

permalink: /ibm/uml2/
excerpt: "Unsupervised Machine Learning"
last_modified_at: Now

toc: true
toc_sticky: true
katex: true
---

## Distant Metrics
Choice of Metrics?

### Euclidean Distance, L2 distnace

$${\displaystyle d(p,q)={\sqrt {(p_{1}-q_{1})^{2}+(p_{2}-q_{2})^{2}+\cdots +(p_{i}-q_{i})^{2}+\cdots +(p_{n}-q_{n})^{2}}}.}$$

#### [L2 norm](https://medium.com/mlearning-ai/is-l2-norm-euclidean-distance-a9c04be0b3ca)

$${\displaystyle \|{\boldsymbol {x}}\|_{2}:={\sqrt {x_{1}^{2}+\cdots +x_{n}^{2}}}.}$$

### Cosine Distance
Cosine is better for data such as text where location of occurence is less importance. Also, it is more robust than euclidean distance, which is vulnerable in multi-dimension.

$$cos(\theta)=\frac{\vec{x} \cdot \vec{y}}{\|\vec{x}\| \|\vec{y}\|}$$

### Jaccards Distance
![Jaccard](/assets/images/IBM/Jaccard.png)

## Clustering Algorithms
