---
title: "Clustering Algorithms"

category: IBM Machine Learning
tags:
  - [Unsupervised, Week2]

layout: single_v2

permalink: /ibm/uml2/
excerpt: "Unsupervised Machine Learning"
last_modified_at: Now

toc: true
toc_sticky: true
katex: true
---

## Distant Metrics
Choice of Metrics?

### Manhattan Distance, L1 distance

### Euclidean Distance, L2 distnace

$${\displaystyle d(p,q)={\sqrt {(p_{1}-q_{1})^{2}+(p_{2}-q_{2})^{2}+\cdots +(p_{i}-q_{i})^{2}+\cdots +(p_{n}-q_{n})^{2}}}.}$$

#### [L2 norm](https://medium.com/mlearning-ai/is-l2-norm-euclidean-distance-a9c04be0b3ca)

$${\displaystyle \|{\boldsymbol {x}}\|_{2}:={\sqrt {x_{1}^{2}+\cdots +x_{n}^{2}}}.}$$

### Cosine Distance
Cosine is better for data such as text where location of occurence is less importance. Also, it is more robust than euclidean distance, which is vulnerable in multi-dimension. Cosine is better for data such as text where location of occurrence is less important.

$$cos(\theta)=\frac{\vec{x} \cdot \vec{y}}{\|\vec{x}\| \|\vec{y}\|}$$

### Jaccards Distance
![Jaccard](/assets/images/IBM/Jaccard.png)

## Clustering Algorithms

### Hierarchical Agglomerative Clustering
1. Find closet pair and merge them.
2. We get clusters and regard it as a point. For distance between clusters, use average distance regarding all the point within their respective clusters.
3. Repeat 1 and 2 until we get a single cluster.

#### Average cluster distance
For the new combined cluster, cluster distance will be increased. Some cluster will merged with larger value of cluster distance. When all the distance is above a threshold, we stop clustering.

#### Single linkage
Minimum pairwise distance between clusters
**Pro** is a clear separation of clusters. **Con** is that seperation is vague with outliers failing close to certain clusters.

#### Complete linkage
We take maximum value.
**Pro** better seperating even with noise. **Cons** tend to break apart big clusters. 

#### Average linkage
Both Pro and Cons of Single and Complete

#### Ward linkage
merge based on best inertia.

### Code
```python
from skleran.cluster import AgglomerativeClustering
agg = AgglomerativeClustering(n_clusters=2, 
                              affinity='euclidean', 
                              linkage='average')
agg.fit(X)
y_predict = agg.predict(test)
```

